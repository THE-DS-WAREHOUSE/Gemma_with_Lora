{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:37:03.009578Z",
     "start_time": "2024-09-20T01:36:56.096764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os # libray to access Operative System, in this case it was only use to access environment variable that contains HF API key\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # transformers contains Tokenizer and LLM (in this case gemma-2b)\n",
    "import torch # torch enables CUDA (for GPU processing) https://pytorch.org/ you can pip install over here (make sure to check your GPU version first\n",
    "# using command in cmd\n",
    "# WARNING: if you dont have a GPU available on your PC or Laptop better run this code in Colab (Code does not work if you don't have GPU)\n",
    "from huggingface_hub import HfApi # to access hugging face API (contains dataset to train)"
   ],
   "id": "8b095098bec424a9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:37:03.088120Z",
     "start_time": "2024-09-20T01:37:03.009578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # we check GPU availability\n",
    "if torch.cuda.is_available(): # if True\n",
    "    print(\"We can use GPU\") # we can use GPU\n",
    "else:\n",
    "    print(\"We don't use GPU\") # we cannot use GPU"
   ],
   "id": "ed645163b05cbc47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use GPU\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1 - Connect to Hugging Face API",
   "id": "9ca35ba132a332fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:37:03.103774Z",
     "start_time": "2024-09-20T01:37:03.088120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api = HfApi(token=os.environ.get('HF_TOKEN')) # you can get API Token by creating an HF account https://huggingface.co --> Setting --> tokens\n",
    "# signed-up users have 100 request per hour"
   ],
   "id": "4c3d22ecae94fcc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2 - Load Model",
   "id": "c5a9cf58fee8754b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:37:15.708130Z",
     "start_time": "2024-09-20T01:37:03.103774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/gemma-2b\"  # Replace with a model you have access (you need ask for permission to access gemma-2b) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = os.environ.get('HF_TOKEN')) # load tokenizer using your HF API Key (tokenizer is linked to model name, so you do not need to worry if it is the correct one)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token = os.environ.get('HF_TOKEN')) # load model using your HF API key\n",
    "# is going to take a while to download (model is about 5 GB), once downloaded it would compile faster in future runs\n",
    "#print(model) # Print the model if you want to take a look for 'proj' layers those are the ones that we can adjust using LoRA (Low Rank Adaptation) these are know as attention layers"
   ],
   "id": "66bfa0c5a106a7aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bdaa1b4759e4ee3add45892aef4032a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First Question",
   "id": "1ed448f6c24afe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:38:09.883340Z",
     "start_time": "2024-09-20T01:37:22.846500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"What should I do on a trip to Europe?\" # prompt\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids, max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "99711c934d264e3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What should I do on a trip to Europe?\n",
      "\n",
      "The answer to this question is not as simple as it seems. There are many different things to see and do in Europe, and it can be difficult to know where to start.\n",
      "\n",
      "If youâ€™re planning a trip to Europe, here are some tips to help you get started:\n",
      "\n",
      "1. Decide what you want to see and do.\n",
      "\n",
      "There are so many amazing places to see and do in Europe, it can be hard to know where to start. Start by deciding what you want to see and do. Do you want to see the Eiffel Tower in Paris? Or\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "As you can see the answer from the model is not that quite relevant, however the point of Fine-Tuning is to adjust does attention layers to obtain better results"
   ],
   "id": "9c6d6f1d1b79f891"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Second Question",
   "id": "6988618ae1290ef8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:38:52.820881Z",
     "start_time": "2024-09-20T01:38:09.883340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Explain the process of photosynthesis in a way that a child could understand\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids, max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4323705fde22d6c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Explain the process of photosynthesis in a way that a child could understand.\n",
      "\n",
      "A 100-W lightbulb is plugged into a standard $120-\\mathrm{V}$ (rms) outlet. Find $(a) I_{\\text {mas }}$ and $(b) I_{\\max }$ when a device like this is operating at the maximum current allowed by its own internal circuitry. (Such a device is often called a light dimmer.)\n",
      "\n",
      "A 100-turn, 2.0-cm-diameter coil is at rest with its axis vertical. A uniform magnetic field $60^{\\circ}$ away\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "In this example the answer is not very clear, and it is kind of confusing, the point of Fine-Tuning is to fix this kind of issues."
   ],
   "id": "a301e75a36f465a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3 - Check Dataset (dolly-15k)",
   "id": "ba0a8c19dc51d4d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:39:55.678776Z",
     "start_time": "2024-09-20T01:39:54.533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset # to get datasets \n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\" # opensource datasets that has several kind of inputs to LLM\n",
    "dataset = load_dataset(dataset_name, split=\"train[0:1000]\") # just take the first 1000 records\n",
    "\n",
    "print(f\"Instruction is: {dataset[0]['instruction']}\") # take a look of the data\n",
    "print(f\"Response is: {dataset[0]['response']}\") # response \n",
    "print(f\"context is: {dataset[0]['context']}\") # context (not all rows has it)\n",
    "print(f\"category is: {dataset[0]['category']}\")  # category of the question\n",
    "dataset"
   ],
   "id": "fb9ce9d665a91e02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction is: When did Virgin Australia start operating?\n",
      "Response is: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n",
      "context is: Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\n",
      "category is: closed_qa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4 - Finetune using Parameter Efficient Fine-tuning (PEFT)\n",
    "- if you don't use PEFT then you will overload GPU and training might be impossible (remember that Gemma-2b has 2 billion parameters)"
   ],
   "id": "6d07efcbf64600bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Do all the imports",
   "id": "cf8ab9efded33207"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:39:59.466553Z",
     "start_time": "2024-09-20T01:39:58.964457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch # to modify the model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, # to have access to the model\n",
    "    AutoTokenizer, # to have access to the model tokenizer\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ],
   "id": "78be8ee0c232fc35",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5 - Load Dataset",
   "id": "f7e9860f7143cef5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:40:26.433103Z",
     "start_time": "2024-09-20T01:40:25.108327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\") #load whole dataset"
   ],
   "id": "d87fea96fe2c47b9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check record categories",
   "id": "43db4a82a62b63e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:40:33.794697Z",
     "start_time": "2024-09-20T01:40:33.015482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "categories_count = defaultdict(int)\n",
    "for __, data in enumerate(dataset):\n",
    "    categories_count[data['category']] += 1\n",
    "print(categories_count)"
   ],
   "id": "4d22cf71f8e78e34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'closed_qa': 1773, 'classification': 2136, 'open_qa': 3742, 'information_extraction': 1506, 'brainstorming': 1766, 'general_qa': 2191, 'summarization': 1188, 'creative_writing': 709})\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 6 - Filter data",
   "id": "6d54b0b8d1879b56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:40:42.178367Z",
     "start_time": "2024-09-20T01:40:41.292384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# exclude those that do not have any context\n",
    "filtered_dataset = []\n",
    "for __, data in enumerate(dataset):\n",
    "    if data[\"context\"]:\n",
    "        continue\n",
    "    else:\n",
    "        text = f\"Instruction:\\n{data['instruction']}\\n\\nResponse:\\n{data['response']}\"\n",
    "        filtered_dataset.append({\"text\": text})\n",
    "\n",
    "print(filtered_dataset[0:2]) #check the filtered data"
   ],
   "id": "d3af28aebe1187d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Instruction:\\nWhich is a species of fish? Tope or Rope\\n\\nResponse:\\nTope'}, {'text': 'Instruction:\\nWhy can camels survive for long without water?\\n\\nResponse:\\nCamels use the fat in their humps to keep them filled with energy and hydration for long periods of time.'}]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 7 - Create new split file",
   "id": "1c35503f44f35550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:23.167689Z",
     "start_time": "2024-09-19T01:29:23.073944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert to json and save the filtered dataset as jsonl file\n",
    "import jsonlines as jl\n",
    "with jl.open('dolly-mini-train-learning.jsonl', 'w') as writer:\n",
    "    writer.write_all(filtered_dataset[0:])"
   ],
   "id": "a54fc8e2ffb1b3eb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 8 - Load Filtered data from HF API",
   "id": "7deea0705f36e444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:40:54.085891Z",
     "start_time": "2024-09-20T01:40:52.238240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset # access to HF datasets\n",
    "# create your own repository of data with your username and repository name, mine is private, so you won't be able to enter\n",
    "dataset_name = \"joselopez1999/data_bricks_train_learning\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[0:1500]\") # adjust the ammount of data you want to load (my case, 1500)\n",
    "dataset"
   ],
   "id": "33b9165bc45bcba1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1500\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 9 - Define all the parameters\n",
    "- LoRA parameters\n",
    "- bitsandbytes parameters\n",
    "- training arguments / parameters\n",
    "- Supervised fine-tuning (SFT) parameters"
   ],
   "id": "42ce4a715e2d1e95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:40:59.855168Z",
     "start_time": "2024-09-20T01:40:59.824005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define some variables - model names\n",
    "model_name = \"google/gemma-2b\" # base LLM to Fine tune\n",
    "new_model = \"gemma-ft\" #new model nam,e\n",
    "\n",
    "# LoRA parameters\n",
    "# LoRA attention dimension\n",
    "lora_r = 8 #if you have a huge GPU you can try to use higher values (lower values consume less memory)\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16 #scales the update of LoRA matrices (it determines the impact or weight new model has compared to the one is being tuned)\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1 # probability to random drop connections between layers (improves learning)\n",
    "\n",
    "# bitsandbytes parameters\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False # disable double quantization\n",
    "\n",
    "# TrainingArguments parameters\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\" # to visualize performance latter\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1 # just one epoch to run once\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False # this depends on hardware\n",
    "bf16 = False # if you have more than 8 cores available on GPU you can use it as True (I have 6, so I won't use it)\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4 # huge batch_size can overload the memory, try with lower values (this depends on hardware)\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4 # huge batch_size can overload the memory, try with lower values (this depends on hardware)\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3 # this is to avoid exploding gradients\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4 # learning rate of the model\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001 # decay of the model\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1 # max_steps are the same as epochs, -1 it means it is not going to be activated\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03 \n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25 # every how many steps you capture a checkpoint\n",
    "# Log every X updates steps\n",
    "logging_steps = 25 # update steps every 25 steps\n",
    "\n",
    "\n",
    "# SFT parameters\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 64 # this can overload memory, try with low values first\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True # False\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0} # if you now your GPU core you can use this or if you have more than one GPU you can select which to use\n",
    "device_map=\"auto\" # this ensures you use all the GPUs you have available "
   ],
   "id": "bb147da35505b51b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 10 - QLoRA Configuration",
   "id": "10dd400906096b4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:41:09.192591Z",
     "start_time": "2024-09-20T01:41:09.161342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit, # Activates 4-bit precision loading\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type, # nf4\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # float16\n",
    "    bnb_4bit_use_double_quant=use_nested_quant, # False\n",
    ")"
   ],
   "id": "23d51713728f8328",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:41:10.574213Z",
     "start_time": "2024-09-20T01:41:10.542968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"Setting BF16 to True\")\n",
    "        bf16 = True\n",
    "    else:\n",
    "        bf16 = False"
   ],
   "id": "b6fde3888d7cc5d0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 11 - Load Model to Fine-Tune",
   "id": "a2486baea07e2f9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:41:36.809613Z",
     "start_time": "2024-09-20T01:41:31.379774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ.get('HF_TOKEN'),\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          token=os.environ.get('HF_TOKEN'),\n",
    "                                          trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ],
   "id": "7ed5642626f6787",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4d2f18bb0ae4ce0b9c71e3b45727879"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 12 - LoRA Configuration",
   "id": "2a1f19420c5f1aa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:41:39.562086Z",
     "start_time": "2024-09-20T01:41:39.546464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"] #attentions modules to fine tune\n",
    ")"
   ],
   "id": "54d101ab117eb51c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# # Step 13 - Training Arguments",
   "id": "3f4e05b0c93c0db5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:41:41.491715Z",
     "start_time": "2024-09-20T01:41:41.413554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ],
   "id": "43d7f3e16394faab",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 14 - Supervised Fine-Tuning",
   "id": "a4801225980806aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:41:47.310886Z",
     "start_time": "2024-09-20T01:41:45.296531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # formatting_func=format_prompts_fn,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ],
   "id": "8b0a10067ba615df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69fe6a6684534220b69aa6af8d1a8589"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 15 - Training Process",
   "id": "a4a1438244d6fcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:51:57.556553Z",
     "start_time": "2024-09-20T01:41:54.381813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)"
   ],
   "id": "862a8ce51dad1a99",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='616' max='616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [616/616 09:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.814500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.547600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 16 - Visualize results of Fine-Tuning",
   "id": "16fbfc9ce7a4c0d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:53:26.180996Z",
     "start_time": "2024-09-20T01:53:21.973846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ],
   "id": "1a3ed6a55c956634",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "80b48be374483d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 17 - Prompt the newly fine-tuned model\n",
    "* Load and MERGE the LoRA weights with the model weights\n",
    "* Run inference with the same prompt we used to test the pre-trained model"
   ],
   "id": "b1d778e809a79b46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:54:30.441278Z",
     "start_time": "2024-09-20T01:54:18.290591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"What are some of the best places to visit in Europe?\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "id": "882caa6a15ef828d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e18f50e0075d49bfae8d22d973100403"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First Question after Fine-Tuning",
   "id": "34a016f38bfe0d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:56:21.210380Z",
     "start_time": "2024-09-20T01:55:32.023723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "343e0f1ab51c3f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What are some of the best places to visit in Europe?\n",
      "\n",
      "Response:\n",
      "Europe is a continent that is home to many countries and cities. Some of the best places to visit in Europe include Paris, London, Rome, Barcelona, Amsterdam, Venice, Prague, Berlin, Copenhagen, Vienna, Lisbon, and many more. Each city has its own unique culture, history, and attractions that make it a must-visit destination.<eos>\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
