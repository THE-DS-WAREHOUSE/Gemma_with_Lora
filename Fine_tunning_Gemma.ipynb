{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:50:14.688710Z",
     "start_time": "2024-09-19T02:50:14.673135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os # libray to access Operative System, in this case it was only use to access environment variable that contains HF API key\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM # transformers contains Tokenizer and LLM (in this case gemma-2b)\n",
    "import torch # torch enables CUDA (for GPU processing) https://pytorch.org/ you can pip install over here (make sure to check your GPU version first\n",
    "# using command in cmd\n",
    "# WARNING: if you dont have a GPU available on your PC or Laptop better run this code in Colab (Code does not work if you don't have GPU)\n",
    "from huggingface_hub import HfApi # to access hugging face API (contains dataset to train)"
   ],
   "id": "8b095098bec424a9",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:27:35.055080Z",
     "start_time": "2024-09-19T01:27:35.039433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # we check GPU availability\n",
    "if torch.cuda.is_available(): # if True\n",
    "    print(\"We can use GPU\") # we can use GPU\n",
    "else:\n",
    "    print(\"We don't use GPU\") # we cannot use GPU"
   ],
   "id": "ed645163b05cbc47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use GPU\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connect to Hugging Face API",
   "id": "9ca35ba132a332fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T02:58:18.702180Z",
     "start_time": "2024-09-19T02:58:18.677013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api = HfApi(token=os.environ.get('HF_TOKEN')) # you can get API Token by creating an HF account https://huggingface.co --> Setting --> tokens\n",
    "# signed-up users have 100 request per hour"
   ],
   "id": "4c3d22ecae94fcc",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:27:48.233356Z",
     "start_time": "2024-09-19T01:27:39.999573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"google/gemma-2b\"  # Replace with a model you have access (you need ask for permission to access gemma-2b) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token = os.environ.get('HF_TOKEN')) # load tokenizer using your HF API Key\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token = os.environ.get('HF_TOKEN')) # load model using your HF API key\n",
    "# is going to take a while to download (model is about 5 GB), once downloaded it would compile faster in future runs\n",
    "print(model) # Print the model if you want to take a look for 'proj' layers those are the ones that we can adjust using LoRA (Low Rank Adaptation)"
   ],
   "id": "66bfa0c5a106a7aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5a88dd187034b9d9c8f1a242d768ce5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First Question",
   "id": "1ed448f6c24afe3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:28:35.229474Z",
     "start_time": "2024-09-19T01:27:48.234319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"What should I do on a trip to Europe?\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids, max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "99711c934d264e3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What should I do on a trip to Europe?\n",
      "\n",
      "The answer to this question is not as simple as it seems. There are many different things to see and do in Europe, and it can be difficult to know where to start.\n",
      "\n",
      "If youâ€™re planning a trip to Europe, here are some tips to help you get started:\n",
      "\n",
      "1. Decide what you want to see and do.\n",
      "\n",
      "There are so many amazing places to see and do in Europe, it can be hard to know where to start. Start by deciding what you want to see and do. Do you want to see the Eiffel Tower in Paris? Or\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Second Question",
   "id": "6988618ae1290ef8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:17.223174Z",
     "start_time": "2024-09-19T01:28:35.229474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Explain the process of photosynthesis in a way that a child could understand\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "outputs = model.generate(**input_ids, max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4323705fde22d6c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,  74198,    573,   2185,    576, 105156,    575,    476,   1703,\n",
      "            674,    476,   2047,   1538,   3508]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<bos>Explain the process of photosynthesis in a way that a child could understand.\n",
      "\n",
      "A 100-W lightbulb is plugged into a standard $120-\\mathrm{V}$ (rms) outlet. Find $(a) I_{\\text {mas }}$ and $(b) I_{\\max }$ when a device like this is operating at the maximum current allowed by its own internal circuitry. (Such a device is often called a light dimmer.)\n",
      "\n",
      "A 100-turn, 2.0-cm-diameter coil is at rest with its axis vertical. A uniform magnetic field $60^{\\circ}$ away\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2 - Motivation for PEFT\n",
    "- Try to finetune Gemma on Dolly dataset without LoRA\n",
    "  * Load and visualize the dataset\n",
    "  * Initiate the trainer\n",
    "  * Start the training\n",
    "  * Training with one single GPU my overload memory"
   ],
   "id": "ba0a8c19dc51d4d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:20.443586Z",
     "start_time": "2024-09-19T01:29:17.223174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[0:1000]\")\n",
    "\n",
    "print(f\"Instruction is: {dataset[0]['instruction']}\")\n",
    "print(f\"Response is: {dataset[0]['response']}\")\n",
    "dataset"
   ],
   "id": "fb9ce9d665a91e02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction is: When did Virgin Australia start operating?\n",
      "Response is: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Finetune using Parameter Efficient Fine-tuning (PEFT)\n",
    "- Create your own dataset\n",
    "  - Visualizing the dataset\n",
    "  - Cleaning and Preprocessing\n",
    "  - Uploading to HF hub\n",
    "- Fine-tune with the dataset"
   ],
   "id": "6d07efcbf64600bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Do all the imports",
   "id": "cf8ab9efded33207"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:20.818520Z",
     "start_time": "2024-09-19T01:29:20.443586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ],
   "id": "78be8ee0c232fc35",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create own dataset (to optimize training)",
   "id": "f7e9860f7143cef5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:21.881682Z",
     "start_time": "2024-09-19T01:29:20.818520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ],
   "id": "d87fea96fe2c47b9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check information from dataset",
   "id": "43db4a82a62b63e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:22.479941Z",
     "start_time": "2024-09-19T01:29:21.881682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "categries_count = defaultdict(int)\n",
    "for __, data in enumerate(dataset):\n",
    "    categries_count[data['category']] += 1\n",
    "print(categries_count)"
   ],
   "id": "4d22cf71f8e78e34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'closed_qa': 1773, 'classification': 2136, 'open_qa': 3742, 'information_extraction': 1506, 'brainstorming': 1766, 'general_qa': 2191, 'summarization': 1188, 'creative_writing': 709})\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:23.073944Z",
     "start_time": "2024-09-19T01:29:22.479941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filter out those that do not have any context\n",
    "filtered_dataset = []\n",
    "for __, data in enumerate(dataset):\n",
    "    if data[\"context\"]:\n",
    "        continue\n",
    "    else:\n",
    "        text = f\"Instruction:\\n{data['instruction']}\\n\\nResponse:\\n{data['response']}\"\n",
    "        filtered_dataset.append({\"text\": text})\n",
    "\n",
    "print(filtered_dataset[0:2])"
   ],
   "id": "d3af28aebe1187d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Instruction:\\nWhich is a species of fish? Tope or Rope\\n\\nResponse:\\nTope'}, {'text': 'Instruction:\\nWhy can camels survive for long without water?\\n\\nResponse:\\nCamels use the fat in their humps to keep them filled with energy and hydration for long periods of time.'}]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create new splitted file",
   "id": "1c35503f44f35550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:23.167689Z",
     "start_time": "2024-09-19T01:29:23.073944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert to json and save the filtered dataset as jsonl file\n",
    "import jsonlines as jl\n",
    "with jl.open('dolly-mini-train-learning.jsonl', 'w') as writer:\n",
    "    writer.write_all(filtered_dataset[0:])"
   ],
   "id": "a54fc8e2ffb1b3eb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data from HF API",
   "id": "7deea0705f36e444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:24.874003Z",
     "start_time": "2024-09-19T01:29:23.167689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"joselopez1999/data_bricks_train_learning\"\n",
    "dataset = load_dataset(dataset_name, split=\"train[0:1000]\")\n",
    "dataset"
   ],
   "id": "33b9165bc45bcba1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define all the parameters\n",
    "- LoRA parameters\n",
    "- bitsandbytes parameters\n",
    "- training arguments / parameters\n",
    "- Supervised fine-tuning (SFT) parameters"
   ],
   "id": "42ce4a715e2d1e95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:29:58.621511Z",
     "start_time": "2024-09-19T01:29:58.609762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define some variables - model names\n",
    "model_name = \"google/gemma-2b\"\n",
    "new_model = \"gemma-ft\"\n",
    "\n",
    "################################################################################\n",
    "# LoRA parameters\n",
    "################################################################################\n",
    "# LoRA attention dimension\n",
    "# lora_r = 64\n",
    "lora_r = 4\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 40 # None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True # False\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "device_map=\"auto\""
   ],
   "id": "bb147da35505b51b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:30:03.892090Z",
     "start_time": "2024-09-19T01:30:03.860944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit, # Activates 4-bit precision loading\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type, # nf4\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # float16\n",
    "    bnb_4bit_use_double_quant=use_nested_quant, # False\n",
    ")"
   ],
   "id": "23d51713728f8328",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:30:10.059266Z",
     "start_time": "2024-09-19T01:30:10.043643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"Setting BF16 to True\")\n",
    "        bf16 = True\n",
    "    else:\n",
    "        bf16 = False"
   ],
   "id": "b6fde3888d7cc5d0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:31:02.266431Z",
     "start_time": "2024-09-19T01:30:55.817895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ.get('HF_TOKEN'),\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          token=os.environ.get('HF_TOKEN'),\n",
    "                                          trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ],
   "id": "7ed5642626f6787",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4719a1e7ad6049669c71e2d3b36e80c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:31:15.916388Z",
     "start_time": "2024-09-19T01:31:15.885285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",
    ")"
   ],
   "id": "54d101ab117eb51c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:31:18.327677Z",
     "start_time": "2024-09-19T01:31:18.247234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "training_arguments"
   ],
   "id": "43d7f3e16394faab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=no,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=True,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=0.0002,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./results\\runs\\Sep18_19-31-18_Alienware-17R5,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=25,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=constant,\n",
       "max_grad_norm=0.3,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=paged_adamw_32bit,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=4,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./results,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=25,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.03,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.001,\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:31:22.786493Z",
     "start_time": "2024-09-19T01:31:20.539527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    # formatting_func=format_prompts_fn,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ],
   "id": "8b0a10067ba615df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc39c9f40ac1448bbc8ec9a35ba6f293"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:41:59.010422Z",
     "start_time": "2024-09-19T01:31:26.891515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)"
   ],
   "id": "862a8ce51dad1a99",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\PycharmProjects\\Gemma_with_Lora\\.venv\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 10:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.922600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.908300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.771700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.932500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.638200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T01:42:25.800559Z",
     "start_time": "2024-09-19T01:42:21.658923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ],
   "id": "1a3ed6a55c956634",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "80b48be374483d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prompt the newly fine-tuned model\n",
    "* Load and MERGE the LoRA weights with the model weights\n",
    "* Run inference with the same prompt we used to test the pre-trained model"
   ],
   "id": "b1d778e809a79b46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:06:07.622225Z",
     "start_time": "2024-09-19T04:05:51.562686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"What are some of the best places to visit in Canada?\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "id": "882caa6a15ef828d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64a69e5406b74b7c9e38c6db7ca060a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T04:13:14.164801Z",
     "start_time": "2024-09-19T04:06:07.623230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids, max_length=128)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "343e0f1ab51c3f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>What are some of the best places to visit in Canada?\n",
      "\n",
      "Canada is a country that is known for its natural beauty and diverse landscapes. From the bustling cities of Toronto and Vancouver to the rugged wilderness of the Canadian Rockies, there is something for everyone in this vast country. Here are some of the best places to visit in Canada:\n",
      "\n",
      "1. Toronto: Canada's largest city, Toronto is a vibrant and cosmopolitan city with a rich history and a thriving arts and culture scene. From the bustling downtown core to the leafy neighbourhoods of the west end, there is something for everyone in this city. Must-see attractions include the CN\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
